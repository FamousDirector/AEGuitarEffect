{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-8-95f54d826028>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     37\u001B[0m         \u001B[0mnew_waveform\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnew_waveform\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mto\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'cpu'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     38\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 39\u001B[0;31m \u001B[0mplot_waveform\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mwaveform\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msample_rate\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtitle\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"Original\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     40\u001B[0m \u001B[0mplot_waveform\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnew_waveform\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msample_rate\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtitle\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"Effects Applied\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     41\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/AEGuitarEffect/utils.py\u001B[0m in \u001B[0;36mplot_waveform\u001B[0;34m(waveform, sample_rate, title, xlim, ylim)\u001B[0m\n\u001B[1;32m     24\u001B[0m   \u001B[0mwaveform\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mwaveform\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnumpy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     25\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 26\u001B[0;31m   \u001B[0mnum_channels\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnum_frames\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mwaveform\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     27\u001B[0m   \u001B[0mtime_axis\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0marange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnum_frames\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m/\u001B[0m \u001B[0msample_rate\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     28\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mValueError\u001B[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "\n",
    "from utils import *\n",
    "\n",
    "input_size = 1000\n",
    "\n",
    "input_directory = \"input_data/\"\n",
    "output_directory = \"generated_sounds/\"\n",
    "\n",
    "model = torch.load(\"saved_models/model.pth\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "for file in os.listdir(input_directory):\n",
    "    if file.endswith(\".wav\"):\n",
    "        waveform, sample_rate = torchaudio.load(os.path.join(input_directory, file))\n",
    "        waveform = waveform[0]  # mono channel\n",
    "\n",
    "        waveform_chunks = []\n",
    "\n",
    "        i = 0\n",
    "        while len(waveform) > (i + input_size):\n",
    "            w = waveform[i:i + input_size].unsqueeze(0).unsqueeze(0)\n",
    "            w = w.to(device)\n",
    "            o = model(w)\n",
    "            waveform_chunks.append(o.flatten())\n",
    "            i += input_size\n",
    "\n",
    "        new_waveform = torch.cat(waveform_chunks).unsqueeze(0)\n",
    "\n",
    "        waveform = waveform.to('cpu')\n",
    "        new_waveform = new_waveform.to('cpu')\n",
    "\n",
    "        plot_waveform(waveform, sample_rate, title=\"Original\")\n",
    "        plot_waveform(new_waveform, sample_rate, title=\"Effects Applied\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}